{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hungarian tools and corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to work on a different language, we'll help you on your way. Here's a notebook for Hungarian. It's separated in two sections: tools and corpora. The first part shows you how to get a Hungarian corpus, while the second part shows you how to tag sentences.\n",
    "\n",
    "This is just a small sample of resources out there for Hungarian, but these suffice for now. I will extend this notebook as needed (send me an email)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpora\n",
    "\n",
    "There are several Hungarian corpora, but the [Webcorpus](http://mokk.bme.hu/en/resources/webcorpus/) seems the most accessible, and it's big enough to find interesting stuff. There are several versions of the webcorpus, but the only one made available online is the \"4% version\". This means that it contains only 4% of the entire corpus. Still, we're talking about over a million web pages, containing 589 million tokens and over 7 million types. That's a lot of words!\n",
    "\n",
    "These are the files you can download. Start with the first file, and only download the others if you need more data:\n",
    "\n",
    "```\n",
    "web2-4p-0.tar.gz        09-Jun-2004 22:15  365M\n",
    "web2-4p-1.tar.gz        09-Jun-2004 22:22  377M\n",
    "web2-4p-2.tar.gz        09-Jun-2004 22:30  371M\n",
    "web2-4p-3.tar.gz        09-Jun-2004 22:38  373M\n",
    "web2-4p-4.tar.gz        09-Jun-2004 22:46  366M\n",
    "web2-4p-5.tar.gz        09-Jun-2004 22:54  370M\n",
    "web2-4p-6.tar.gz        09-Jun-2004 23:01  372M\n",
    "web2-4p-7.tar.gz        09-Jun-2004 23:09  371M\n",
    "web2-4p-8.tar.gz        09-Jun-2004 23:17  370M\n",
    "web2-4p-9.tar.gz        09-Jun-2004 23:25  375M\n",
    "```\n",
    "\n",
    "I've downloaded `web2-4p-0.tar.gz` and unpacked it, ending up with a folder called `content` that has loads of files in it. Let's print one of the files and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Megöntözöd onnan fentről a hegyeket, alkotásaid gyümölcsével jól tartod a földet.\n",
      "<w>Megöntözöd\n",
      "</w>\n",
      "<w>onnan\n",
      "</w>\n",
      "<w>fentről\n",
      "</w>\n",
      "<w>a\n",
      "</w>\n",
      "<w>hegyeket\n",
      "</w>\n",
      "<c>,</c>\n",
      "<w>alkotásaid\n",
      "</w>\n",
      "<w>gyümölcsével\n",
      "</w>\n",
      "<w>jól\n",
      "</w>\n",
      "<w>tartod\n",
      "</w>\n",
      "<w>a\n",
      "</w>\n",
      "<w>földet\n",
      "</w>\n",
      "<c>.</c>\n",
      "</s>\n",
      "<s>Füvet sarjasztasz az állatoknak, növényeket a földművelő embernek, hogy kenyeret termeljen a földből.\n",
      "<w>Füvet\n",
      "</w>\n",
      "<w>sarjasztasz\n",
      "</w>\n",
      "<w>az\n",
      "</w>\n",
      "<w>állatoknak\n",
      "</w>\n",
      "<c>,</c>\n",
      "<w>növényeket\n",
      "</w>\n",
      "<w>a\n",
      "</w>\n",
      "<w>földművelő\n",
      "</w\n"
     ]
    }
   ],
   "source": [
    "# Note the encoding for this file! There wasn't any note on the website, but it's not utf-8!\n",
    "# I found the right encoding by looking for 'Hungarian encoding' on Google.\n",
    "# Here's one of the pages I found: https://en.wikipedia.org/wiki/ISO/IEC_8859\n",
    "\n",
    "with open('/Users/Emiel/Downloads/content/0000000042', encoding='ISO-8859-2') as f:\n",
    "    # Print the first 500 characters.\n",
    "    print(f.read()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is a kind of minimal, raw XML. Some observations:\n",
    "\n",
    "* The file has no root covering all the sentences.\n",
    "* Each sentence is within `<s>...</s>`-tags.\n",
    "* The sentences are tokenized, with all words in `<w>...</w>`-tags.\n",
    "* There's a separate tag for punctuation: `<c>...</c>`.\n",
    "\n",
    "We cannot parse this kind of XML with a regular XML parser, because there is no root. But this does work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Element s at 0x104d3a4c8>, <Element s at 0x104d54e48>, <Element s at 0x104d54cc8>, <Element s at 0x104d54588>, <Element s at 0x104d54448>, <Element s at 0x104d54fc8>, <Element s at 0x104d543c8>, <Element s at 0x104d54808>, <Element s at 0x104d50e08>, <Element s at 0x104d505c8>, <Element s at 0x104d50f08>, <Element s at 0x104d50f48>, <Element s at 0x104d50ec8>, <Element s at 0x104d3e3c8>, <Element s at 0x104d3e288>, <Element s at 0x104d3e2c8>, <Element s at 0x104d3e308>, <Element s at 0x104d3e748>, <Element s at 0x104d3e788>, <Element s at 0x104d3e7c8>, <Element s at 0x104d3e808>, <Element s at 0x104d3e848>, <Element s at 0x104d3e888>, <Element s at 0x104d3e8c8>, <Element s at 0x104d3e908>, <Element s at 0x104d3e948>, <Element s at 0x104d3e988>, <Element s at 0x104d3e9c8>, <Element s at 0x104d3ea08>]\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "\n",
    "with open('/Users/Emiel/Downloads/content/0000000042', encoding='ISO-8859-1') as f:\n",
    "    corpus_data = f.read()\n",
    "\n",
    "root = etree.HTML(corpus_data)\n",
    "\n",
    "# Somehow etree.HTML introduces P-elements in the corpus. I don't know why.\n",
    "sentences = root.xpath('.//s')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of elements, but what can we do with those?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n"
     ]
    }
   ],
   "source": [
    "# Let's get the first sentence to experiment with.\n",
    "first_sentence = sentences[0]\n",
    "\n",
    "# Get the tag of the first sentence.\n",
    "print(first_sentence.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here's how to get the tokens for the first sentence.\n",
    "tokens = []\n",
    "for token in first_sentence:\n",
    "    # All text is surrounded by newline characters. Strip() removes that.\n",
    "    text = token.text.strip()\n",
    "    tokens.append(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a list of tokens. This is excellent for the second part of the notebook!\n",
    "\n",
    "Some final hints for working with this corpus:\n",
    "\n",
    "* Write a function to open a file, load the data, and return the sentences (where each sentence is a list of tokens).\n",
    "* Use the `glob`-module to create a list of all corpus files.\n",
    "* Advanced students may wish to write a generator function that yields sentences for the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "The second part of this notebook shows you how to use the `hunpos` part-of-speech tagger. This is a multiplatform tagger that you can download from [here](https://code.google.com/archive/p/hunpos/downloads). Please download both the tagger (`hunpos-1.0-...`) and the Hungarian model (`hu_szeged_kr.model.gz`). Unpack both files on your computer.\n",
    "\n",
    "The tagger is pretty easy to use. Here's how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import hunpos\n",
    "\n",
    "# Change these paths for your computer.\n",
    "# On Windows, I suspect you have to use hunpos-tag.exe.\n",
    "path_to_model = '/Users/Emiel/Downloads/hu_szeged_kr.model'\n",
    "path_to_tagger = '/Users/Emiel/Downloads/hunpos-1.0-macosx/hunpos-tag'\n",
    "\n",
    "tagger = hunpos.HunposTagger(path_to_model, path_to_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Megöntözöd', b'NOUN'), ('onnan', b'ADV'), ('fentrõl', b'ADV'), ('a', b'ART'), ('hegyeket', b'NOUN<PLUR><CAS<ACC>>'), (',', b'PUNCT'), ('alkotásaid', b'NUM'), ('gyümölcsével', b'NOUN<CAS<INS>>'), ('jól', b'ADV'), ('tartod', b'VERB<PERS<2>><DEF>'), ('a', b'ART'), ('földet', b'NOUN<CAS<ACC>>'), ('.', b'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentence = tagger.tag(tokens)\n",
    "print(tagged_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only downside right now is that the tags are in bytes. You should get rid of those as soon as possible, to prevent any errors in your program. Here's how to convert to unicode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "bytes_pos = b'NOUN'\n",
    "uni_pos = bytes_pos.decode('utf-8')\n",
    "print(uni_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's probably a good idea to write a *wrapper function* if you plan to use the tagger often. Here's a template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag(tokens, tagger=tagger):\n",
    "    \"\"\"\n",
    "    Wrapper around an instance of HunposTagger.\n",
    "    This function ensures that the tags are in unicode.\n",
    "    \"\"\"\n",
    "    result = tagger.tag(tokens)\n",
    "    # Create a new list to hold the unicode tokens.\n",
    "    # Loop over the result and convert the tags to utf-8.\n",
    "    # HINT: For your loop, remember multiple assignment!\n",
    "    # Return the new result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
