{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "from collections import namedtuple\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a namedtuple because it's nice to work with them.\n",
    "Proceedings = namedtuple('Proceedings', ['year','date','url'])\n",
    "\n",
    "def get_decade_urls():\n",
    "    # Get the main page\n",
    "    main_page = requests.get('https://www.oldbaileyonline.org/browse.jsp?dir=sessionsPapers')\n",
    "    # Decode the HTML to unicode.\n",
    "    main_page = main_page.content.decode('utf-8')\n",
    "    # Parse the HTML\n",
    "    main_page = etree.HTML(main_page)\n",
    "    # Get all the decade urls, looping through the decade list on the page.\n",
    "    base_url = 'https://www.oldbaileyonline.org/'\n",
    "    decade_urls = []\n",
    "    for link in start_page.xpath('//div[@class=\"decadeList\"]/a'):\n",
    "        full_url = base_url + link.attrib['href']\n",
    "        decade_urls.append(full_url)\n",
    "    return decade_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_proceedings_pages(decade_url):\n",
    "    \"Get the links to the proceedings for a given decade.\"\n",
    "    base_url = 'https://www.oldbaileyonline.org/'\n",
    "    proceedings_pages = []\n",
    "    page_for_decade = requests.get(decade_url)\n",
    "    current_page = etree.HTML(example.content.decode('utf-8'))\n",
    "    rows = current_page.xpath('//table[@class=\"dateTable\"]/tr')\n",
    "    for row in rows:\n",
    "        for link in row.xpath('.//a'):\n",
    "            proc_page = Proceedings(year = row[0].text,\n",
    "                                    date = link.text,\n",
    "                                    url = base_url+link.attrib['href'])\n",
    "            proceedings_pages.append(proc_page)\n",
    "    return proceedings_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_items_from_page(page):\n",
    "    \"Function to get all the items from a page.\"\n",
    "    # Get the page URL\n",
    "    page_data = requests.get(page.url)  \n",
    "    # Load the HTML.\n",
    "    current_page = etree.HTML(page_data.content.decode('utf-8'))\n",
    "    # Find the sessionsPaper div, where all the data is stored.\n",
    "    sessionsPaper = current_page.xpath('//div[@class=\"sessionsPaper\"]')[0]\n",
    "    # Turn the div into a string.\n",
    "    sp_as_string = etree.tostring(sessionsPaper)\n",
    "    # Load that into BeautifulSoup, because BS has a nice text method.\n",
    "    soup = BeautifulSoup(sp_as_string,'lxml')\n",
    "    # Get and clean the text.\n",
    "    cleaned = soup.text.replace('See original\\xa0','').replace('\\xa0','')\n",
    "    # Get the title and the rest of the text.\n",
    "    # Each string in rest has its own reference number.\n",
    "    title, *rest = cleaned.split(\"Reference Number: \")\n",
    "    item_dict = dict()\n",
    "    for item in rest:\n",
    "        # Get the reference number.\n",
    "        number, *text = item.split(' ')\n",
    "        # join the text back together.\n",
    "        text = ' '.join(text)\n",
    "        # And normalize the whitespace in the text.\n",
    "        text = re.sub('\\s+', ' ', text).strip()\n",
    "        item_dict[number] = text\n",
    "    return item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_page(page, directory='./', filename=None):\n",
    "    \"Saves page to a file.\"\n",
    "    data = get_items_from_page(page)\n",
    "    # Make a suitable filename\n",
    "    if not filename:\n",
    "        date_nospace = page.date.replace(' ','_')\n",
    "        path = ''.join([directory, page.year, '_', date_nospace, '.tsv'])\n",
    "    else:\n",
    "        path = directory + filename\n",
    "    # Write out to a file\n",
    "    with open(path,'w',encoding='utf-8') as f:\n",
    "        writer=csv.writer(f,delimiter='\\t')\n",
    "        writer.writerows(data.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to get URLs to all the pages.\n",
    "decade_urls = get_decade_urls()\n",
    "durl = decade_urls[0]\n",
    "pages = get_proceedings_pages(durl)\n",
    "# First page is a namedtuple, it is like a tuple, but you can also use first_page.year to get the year etc.\n",
    "first_page = pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you want to get the items on the page as a dictionary, use this:\n",
    "items = get_items_from_page(first_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you want to save the data, use this:\n",
    "save_page(first_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
