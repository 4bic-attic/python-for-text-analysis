{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Getting and Processing data\n",
    "\n",
    "## Due: Tuesday the 22nd of November 2016 15:00 p.m.\n",
    "\n",
    "Please name your Jupyter notebook using the following naming convention: ASSIGNMENT_3_FIRSTNAME_LASTNAME.ipynb \n",
    "\n",
    "Please send your assignment to `emiel.van.miltenburg@vu.nl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hackernews\n",
    "\n",
    "The [Hackernews API](https://github.com/HackerNews/API) is a nice source of discussions between people, and of opiniated text. Those discussions have a tree-like structure, starting with some kind of news item (the *parent*), which gets a couple of responses (*children*), which in turn get responses of their own (more *children*):\n",
    "\n",
    "                                 ,------ Response -------- Response\n",
    "               ,------ Response /------- Response -------- Response\n",
    "    News-item /------- Response -------- Response -------- Response -------- Response     ,------ Response\n",
    "                                                  \\------- Response ----------- Response <------- Response\n",
    "                                                                                          \\------ Response\n",
    "\n",
    "We're going to write some functions to archive specific news items and their comments. At the same time, we'll be using a dictionary to cache all comments and items. Here's a list of the functions to write:\n",
    "\n",
    "* `download_item(key, cache)` downloads an item and returns it as a dictionary.\n",
    "* `download_tree(key, cache)` downloads a tree, starting from the item with identifier `key`.\n",
    "* `download_user(username, cache)` downloads user data.\n",
    "\n",
    "We'll start with [this item](https://news.ycombinator.com/item?id=12926684), a link to rollingstone.com about the death of Leonard Cohen. [Here](https://hacker-news.firebaseio.com/v0/item/12926684.json?print=pretty) is the JSON link from the API. It looks like this:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"by\" : \"joaomsa\",\n",
    "  \"descendants\" : 139,\n",
    "  \"id\" : 12926684,\n",
    "  \"kids\" : [ 12927149, 12926778, 12926940, 12928011, 12927831, 12926845, 12927000, 12928269, 12930113, 12926915, 12927700, 12926985, 12927085, 12927660, 12927349, 12926797, 12926751, 12927372, 12930833, 12928461, 12927964, 12926853, 12926826, 12926900, 12926917, 12927052, 12931153, 12926815, 12927962, 12928757, 12926958, 12926859, 12932982, 12928236, 12927116, 12927287, 12927062, 12930475, 12931778, 12927956, 12928730, 12926891, 12927006, 12926772, 12928207, 12931654, 12926935, 12927048, 12930683, 12927147, 12927375, 12928688, 12931045, 12931424, 12927715, 12929711, 12926941, 12927380, 12928737, 12927781, 12930724, 12927532, 12927786, 12928316, 12935676 ],\n",
    "  \"score\" : 973,\n",
    "  \"time\" : 1478828820,\n",
    "  \"title\" : \"Leonard Cohen Has Died\",\n",
    "  \"type\" : \"story\",\n",
    "  \"url\" : \"http://www.rollingstone.com/music/news/leonard-cohen-dead-at-82-w449792\"\n",
    "}\n",
    "```\n",
    "\n",
    "The first item in `kids`, [a comment](https://hacker-news.firebaseio.com/v0/item/12927149.json?print=pretty), looks like this:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"by\" : \"muhic\",\n",
    "  \"id\" : 12927149,\n",
    "  \"parent\" : 12926684,\n",
    "  \"text\" : \"Sad for the artistic loss but also glad he died at peace after a rich life spent doing what he loved till the last moment. He joins a special list, alongside Hintjens who also passed recently, of those who manage to strip the dread from death and stress the importance of &#x27;tidying up&#x27; over passive acceptance as one enters the final days.<p><i>“The big change is the proximity to death,” he said. “I am a tidy kind of guy. I like to tie up the strings if I can. If I can’t, also, that’s O.K. But my natural thrust is to finish things that I’ve begun.”</i><p><i>“For some odd reason,” he went on, “I have all my marbles, so far. I have many resources, some cultivated on a personal level, but circumstantial, too: my daughter and her children live downstairs, and my son lives two blocks down the street. So I am extremely blessed. I have an assistant who is devoted and skillful. I have a friend like Bob and another friend or two who make my life very rich. So in a certain sense I’ve never had it better. . . . At a certain point, if you still have your marbles and are not faced with serious financial challenges, you have a chance to put your house in order. It’s a cliché, but it’s underestimated as an analgesic on all levels. Putting your house in order, if you can do it, is one of the most comforting activities, and the benefits of it are incalculable.”</i> [0]<p>[0] <a href=\\\"http:&#x2F;&#x2F;www.newyorker.com&#x2F;magazine&#x2F;2016&#x2F;10&#x2F;17&#x2F;leonard-cohen-makes-it-darker\\\" rel=\\\"nofollow\\\">http:&#x2F;&#x2F;www.newyorker.com&#x2F;magazine&#x2F;2016&#x2F;10&#x2F;17&#x2F;leonard-cohen-m...</a>\",\n",
    "  \"time\" : 1478832933,\n",
    "  \"type\" : \"comment\"\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "And we can learn more about the author of this comment by [searching for their name](https://hacker-news.firebaseio.com/v0/user/muhic.json?print=pretty). Here's the result: a JSON file with all their other posts.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"created\" : 1428067410,\n",
    "  \"id\" : \"muhic\",\n",
    "  \"karma\" : 226,\n",
    "  \"submitted\" : [ 12927149, 12927138, 12782022, 12775146, 12775113, 12775075, 12752650, 12720395, 12715272, 12710137, 12641133, 12633663, 12576818, 12576795, 12576783, 12576758, 12576741, 12576730, 12576713, 12572213, 12572168, 12572115, 12563942, 12563846, 12475067, 12474951, 12474893, 12470497, 12426759, 12420573, 12378648, 12328278, 12242845, 12100179, 11928360, 11814853, 11713753, 11713721, 11187226, 11169690, 11064015, 10826141, 10826045, 10806055, 10750122, 10283593, 10272081, 10247645, 9315733 ]\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Before we start, let's create an item cache and a user cache so that we never download data twice. We'll use dictionaries to represent the downloaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_cache = dict() # Keys will be item IDs, values will be dictionaries based on the JSON data.\n",
    "user_cache = dict() # Keys will be usernames, values will be dictionaries based on the JSON data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import the libraries we'll need to download the actual data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The URL for an item looks like this: `https://hacker-news.firebaseio.com/v0/item/12926684.json`. From this, we can see that:\n",
    "\n",
    "* There's a base url: `https://hacker-news.firebaseio.com/v0/item/`\n",
    "* followed by an identifier: `12926684`\n",
    "* and the extension: `.json`\n",
    "\n",
    "**Exercise 1** Write a function, called `download_item(key)` that:\n",
    "\n",
    "* takes an identifier (a string) as its argument\n",
    "* downloads the corresponding item\n",
    "* uses the `json` module to load the JSON data\n",
    "* and then returns the loaded data.\n",
    "\n",
    "Also remember to write a docstring to describe your function.\n",
    "\n",
    "HINT: read through the Recipepuppy example in topic 4!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the function by running this cell:\n",
    "leonard_cohen_article = download_item(\"12926684\")\n",
    "\n",
    "# Store the item in the cache. We don't want to download it again!\n",
    "item_cache[\"12926684\"] = leonard_cohen_article\n",
    "\n",
    "# Print the title of the article.\n",
    "print(leonard_cohen_article[\"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caching**\n",
    "\n",
    "To cache the results, we'll make use of the way variables refer to objects in Python. Here's a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toy_cache = dict()\n",
    "\n",
    "def update_cache(cache):\n",
    "    \"Toy function that adds a test string to a dictionary.\"\n",
    "    fake_item = {\"title\": \"this is a fake item\"}\n",
    "    cache['1'] = fake_item\n",
    "    print(\"We updated the dictionary!\")\n",
    "\n",
    "print(toy_cache)\n",
    "update_cache(toy_cache)\n",
    "print(toy_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the step-by-step execution of the code [here](http://pythontutor.com/visualize.html#code=toy_cache%20%3D%20dict(%29%0A%0Adef%20update_cache(cache%29%3A%0A%20%20%20%20%22Toy%20function%20that%20adds%20a%20test%20string%20to%20a%20dictionary.%22%0A%20%20%20%20fake_item%20%3D%20%7B%22title%22%3A%20%22this%20is%20a%20fake%20item%22%7D%0A%20%20%20%20cache%5B'1'%5D%20%3D%20fake_item%0A%20%20%20%20print(%22We%20updated%20the%20dictionary!%22%29%0A%0Aprint(toy_cache%29%0Aupdate_cache(toy_cache%29%0Aprint(toy_cache%29&cumulative=false&curInstr=9&heapPrimitives=false&mode=display&origin=opt-frontend.js&py=3&rawInputLstJSON=%5B%5D&textReferences=false). What happens is that `toy_cache` initially is made to refer to an empty dictionary. Then, when `update_cache` is called, the variable `cache` is *made to refer to the same dictionary*. This means that `cache['1'] = fake_item` also has an effect on the dictionary that `toy_cache` is referring to.\n",
    "\n",
    "**Exercise 2** Copy/paste the code you've written for `download_item(key)` below. Modify the function so that it has two arguments: `download_item(key, cache)`. The desired behavior of this function is as follows:\n",
    "\n",
    "* When the function is called with a key that is already in the cache, return the associated item.\n",
    "* When the function is called with a key that is not already in the cache, download the relevant item, add it to the cache, and return the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your function, add some `print` functions in both conditions to show that the function behaves differently for these two commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Redefine item_cache to make sure the test still works after running your function multiple times.\n",
    "item_cache = dict()\n",
    "item_cache[\"12926684\"] = leonard_cohen_article\n",
    "\n",
    "# Command 1: This item should already be in the cache:\n",
    "download_item(\"12926684\", item_cache)\n",
    "\n",
    "# Command 2: This item shouldn't be in the cache yet.\n",
    "download_item(\"12927149\", item_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3** Write a function called `download_user(username, cache)`. Its behavior should be similar to `download_item`, only for users. The relevant link is: `https://hacker-news.firebaseio.com/v0/user/USERNAME.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downloading all the replies**\n",
    "\n",
    "Now you've written the basic functions to download items and users, we can write a larger function to collect all the replies to the Leonard Cohen article. But at first glance, it's not very clear how to do that with this kind of structure:\n",
    "\n",
    "                                 ,------ Response -------- Response\n",
    "               ,------ Response /------- Response -------- Response\n",
    "    News-item /------- Response -------- Response -------- Response -------- Response     ,------ Response\n",
    "                                                  \\------- Response ----------- Response <------- Response\n",
    "                                                                                          \\------ Response\n",
    "\n",
    "How do you go through a structure like this? We'll use a to-do list: for the news item, we make a to-do list out of all its responses. These are the items we need to download and store in the cache. But those responses also have responses! So for each response, we extend our to-do list with all its responses. \n",
    "\n",
    "**Exercise 4** You don't have to write the entire function yourself. Just complete `download_tree` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that you don't have to start on the function!\n",
    "# You can also first test pieces of your code here.\n",
    "\n",
    "# E.g. how do you remove one of the elements from a list and make a variable refer to that element?\n",
    "example_list = [1,2,3]\n",
    "removed_element = example_list.some_method()\n",
    "# See if it works\n",
    "print(example_list, removed_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_tree(key, cache):\n",
    "    \"Function that downloads all the responses for a given item, and returns a list of response IDs.\"\n",
    "    # Define a list to collect all the keys.\n",
    "    all_keys = []\n",
    "    root = download_item(key, cache)\n",
    "    \n",
    "    # Here's the to do list. It starts out as a copy of the list of kids from the root.\n",
    "    to_do = root['kids'][:]\n",
    "    \n",
    "    # This loop will keep running until there are no more things to download.\n",
    "    while not len(to_do) == 0:\n",
    "        # YOUR CODE HERE:\n",
    "        # - Use a list method to remove one of the keys from the to do list,\n",
    "        #   and assign that value to a variable called current_key.\n",
    "        # - Add the current key to all_keys.\n",
    "        # - Download the current item.\n",
    "        # - Add its response keys to the to do list.\n",
    "        # - Wait 2 seconds.\n",
    "    return all_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you're able to download all the replies, you can start analyzing the data. It might be a good idea to save the data, so that you can load it in a future session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finish this code to store the item cache on your computer.\n",
    "with open(\"item_cache.json\",'w') as f:\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot we could do with this code and data:\n",
    "\n",
    "* analyze online discussions\n",
    "* create a dataset to see if you can determine who wrote which comment\n",
    "* can you think of more possibilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermezzo: sorting\n",
    "\n",
    "You can use the built-in function `sorted()` ([here](https://docs.python.org/3.6/library/functions.html#sorted)) or `list.sort()` to sort a list. We will use `sorted` as an example. This function takes an iterable and returns it as a sorted list. `list.sort` doesn't return anything, but rather sorts lists in-place. By default, the sorting algorithm determines the ordering based on the first element of whatever you want to sort (in this case tuples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "some_list = [('a',4),('b',6),('c',1),('d',5)]\n",
    "sorted_list = sorted(some_list)\n",
    "\n",
    "# How is the list sorted?\n",
    "print(sorted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And we can sort in reverse order:\n",
    "print(sorted_list, reversed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `sorted` and `list.sort` have a keyword argument called `key`. This determines how the sorting function determines the order. \n",
    "\n",
    "The `key` argument can be any function, which means you can also supply the sorting algorithm with a function to select the *second* element of each object in the list. It's very common to use `lambda`-expressions for this. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_list = sorted(some_list, \n",
    "                     key=lambda item:item[1]) # Use the element with index 1 to sort the list.\n",
    "\n",
    "# How is the list sorted?\n",
    "print(sorted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_list = sorted(some_list, \n",
    "                     key=lambda item:item[0]) # Use the element with index 0 to sort the list.\n",
    "\n",
    "# How is the list sorted?\n",
    "print(sorted_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you can achieve the same thing with the itemgetter() function. Many people find this clearer than using lambdas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "sorted_list = sorted(some_list, \n",
    "                     key=itemgetter(0)) # Use the element with index 0 to sort the list.\n",
    "\n",
    "# How is the list sorted?\n",
    "print(sorted_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis using the NLTK\n",
    "\n",
    "Sometimes authors in NLP implement their findings in the NLTK, so as to make their software available to everyone. This happened with the VADER-tool (Valence  Aware  Dictionary  for sEntiment Reasoning, [Hutto & Gilbert 2014](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf), full code [here](http://www.nltk.org/_modules/nltk/sentiment/vader.html)) As the name suggests, you can use this tool to determine the sentiment of a text. Reading the paper, you can see the algorithm itself is really simple, but it manages to capture sentiment pretty well. (And yes, we know the acronym is horrible.)\n",
    "\n",
    "In this assignment, we'll put this tool to the test. First you need to install it through the NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure you're connected to the internet, and download the sentiment lexicon first.\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the sentiment analyzer class.\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# And instantiate a sentiment analyzer object.\n",
    "# NOTE: this will produce a warning, but you can safely ignore it.\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do you use this tool? Basically, you can just call it with any sentence you like! Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = [\"I am sad.\",\n",
    "             \"I am quite happy.\"]\n",
    "\n",
    "print('--------')\n",
    "for sentence in sentences:\n",
    "    scores = sid.polarity_scores(sentence)\n",
    "    print(sentence)\n",
    "    print(scores)\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**\n",
    "\n",
    "Of course this is a really simple example. You can probably think of some more challenging phrases to enter. Before we'll use the sentiment analyzer to answer some research questions, let's first get some intuitions about what the tool does well, and where it makes mistakes. Try to come up with **two or three** challenging examples (hint: statements with a non-literal meaning) for the tool to judge, and write down your findings.\n",
    "\n",
    "(This is just to get a sense of what the tool does. Don't spend too much time on this exercise!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6** Use the VADER sentiment analysis tool to answer the following two questions:\n",
    "\n",
    "1. What are the top-5 most positive comments in the Hackernews story about Leonard Cohen?\n",
    "2. What are the top-5 most negative comments in the Hackernews story about Leonard Cohen?\n",
    "\n",
    "Some hints:\n",
    "* You can loop over the items in `item_cache` by using `for key, item in item_cache.items(): ...`\n",
    "* You might need an additional list or dictionary to store the sentiment scores for each item.\n",
    "* If you have a list of sorted scores, you can use the list slicing methods to get the first or last N items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User profiling\n",
    "\n",
    "Let's see if we can do some user profiling, to judge what topics people like to talk about. We'll compare the posts of:\n",
    "\n",
    "* Stephen Merity: a machine learning researcher who also has a nice tech blog ([json link here](https://hacker-news.firebaseio.com/v0/user/Smerity.json?print=pretty))\n",
    "* Gabriel Weinberg: founder of the DuckDuckGo search engine ([json link here](https://hacker-news.firebaseio.com/v0/user/epi0Bauqu.json?print=pretty))\n",
    "\n",
    "We expect that Stephen will talk more about technology, and Gabriel will talk more about the Web.\n",
    "\n",
    "**Step 1: download the user data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First download the user profiles.\n",
    "merity = download_user('Smerity', user_cache)\n",
    "weinberg = download_user('epi0Bauqu', user_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: save the user data**\n",
    "\n",
    "This is useful because:\n",
    "\n",
    "* Your analysis will be replicable. (Maybe a user will delete their posts, or start posting about something completely different.)\n",
    "* You won't have to download the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# WARNING: this will overwrite any existing file!\n",
    "with open('user_cache.json','w') as f:\n",
    "    json.dump(f, user_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: create a small corpus**\n",
    "\n",
    "We need to have some data in order to compare the two users. But we don't need to download *all* of their posts. Let's just take the last 10. That should be enough text to get at least some idea of what these two guys post about.\n",
    "\n",
    "**Exercise 7** Write a for-loop to download the last 10 posts for both users. Please make sure that you use `time.sleep` to pause between download requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE.\n",
    "\n",
    "# Make two lists with the 10 post IDs:\n",
    "merity_posts = # YOUR CODE HERE\n",
    "weinberg_posts = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And store the data as well\n",
    "with open(\"item_cache.json\",'w') as f:\n",
    "    json.dump(f, item_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: process the corpus**\n",
    "\n",
    "We now need to process the corpus such that we can actually make a nice comparison between the two users. We will:\n",
    "\n",
    "* Use SpaCy to tokenize and lemmatize the posts by Merity and Weinberg.\n",
    "* Make two lists: `merity_lemmas` and `weinberg_lemmas`.\n",
    "* Use sets to determine the words *exclusively* used by Merity and Weinberg.\n",
    "* Use log-likelihood to determine which words are more likely to be used by either user.\n",
    "\n",
    "Here's some code to remind you how SpaCy works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the SpaCy module.\n",
    "from spacy.en import English\n",
    "\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example:\n",
    "doc = nlp(\"Here is an example.\")\n",
    "\n",
    "# Making a list of strings:\n",
    "orth_tokens = []\n",
    "for token in doc:\n",
    "    # Note the underscore after orth. This is because SpaCy has two representations for words.\n",
    "    # Internally, it has a numbered vocabulary, and you can get the index using `token.orth`.\n",
    "    # And you can get the words as we know them, as sequences of characters, using `token.orth_`\n",
    "    orth_tokens.append(token.orth_)\n",
    "\n",
    "print(orth_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Every doc is represented as a collection of tokens.\n",
    "\n",
    "# Let's coerce it to a list.\n",
    "tokens = list(doc)\n",
    "\n",
    "# Get the first token\n",
    "first_token = tokens[0]\n",
    "\n",
    "# See what we can do with the token:\n",
    "dir(first_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8** Create a function that takes a string (the text of a comment), and returns a list of lemmas. Don't forget to use a docstring to say what the function does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_lemmas(text):\n",
    "    # Don't forget to write a docstring!\n",
    "    # YOUR CODE HERE.\n",
    "    \n",
    "    return list_of_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get this function for free :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_lemma_list(post_ids):\n",
    "    \"Function to generate a lemma list using post ids.\"\n",
    "    # This is the list that the function will use to collect all the lemmas.\n",
    "    lemma_list = []\n",
    "    for key in post_ids:\n",
    "        # First get the post:\n",
    "        post = item_cache[key]\n",
    "        # Get the text from the post:\n",
    "        text = post[\"text\"]\n",
    "        # Get the lemmas for the current text.\n",
    "        lemmas = text_to_lemmas(text)\n",
    "        # Extend the list with the lemmas found for the current text.\n",
    "        lemma_list.extend(lemmas)\n",
    "    return lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all the lemmas \n",
    "merity_lemmas = generate_lemma_list(merity_posts)\n",
    "weinberg_lemmas = generate_lemma_list(weinberg_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log likelihood\n",
    "\n",
    "We are going to learn about *log likelihood*, and implement this measure in order to see which words are unexpected given the null hypothesis that each keyword is equally likely to occur in each corpus (lemmas used by the authors).\n",
    "\n",
    "It's very easy to find words that are only used by one person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute this cell.\n",
    "\n",
    "# Create sets of lemmas.\n",
    "merity_set = set(merity_lemmas)\n",
    "weinberg_set = set(weinberg_lemmas) \n",
    "\n",
    "# See which words are unique to each author.\n",
    "only_merity = merity_set - weinberg_set\n",
    "only_weinberg = weinberg_set - merity_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can play around with the data in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But more interesting are those words that are used by both authors. Let's see how many words those are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overlap = merity_set & weinberg_set\n",
    "print(len(overlap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how do we find out which word is more typical for which author? We will implement log likelihood function based on [this page](http://ucrel.lancs.ac.uk/llwizard.html) by Paul Rayson. First some theory.\n",
    "\n",
    "Log likelihood is a measure of 'unexpectedness' of a word in a particular corpus. We can compute it by comparing two corpora. We expect any given word to occur about the same amount of times in both corpora. If a word occurs in one corpus much more than in some other corpus, we get a high log likelihood score.\n",
    "\n",
    "To compute the log likelihood, we need to build a contingency table:\n",
    "\n",
    "<table bgcolor=\"#eeeeee\" border=1>\n",
    "<tr bgcolor=\"#cccccc\"><td></td><td>Corpus 1</td><td>Corpus 2</td><td>Total</td></tr>\n",
    "<tr><td bgcolor=#cccccc>Frequency of word</td><td>a</td><td>b</td><td>a+b</td></tr>\n",
    "<tr><td bgcolor=#cccccc>Frequency of other words</td><td>c-a</td><td>d-b</td><td>c+d-a-b</td></tr>\n",
    "<tr><td bgcolor=#cccccc>Total</td><td>c</td><td>d</td><td>c+d</td></tr>\n",
    "</table>\n",
    "\n",
    "Now the first step to compute the log likelihood score for a particular word (e.g. 'noise'), is to compute the 'expected values' for that word. We can do this using the following formulae:\n",
    "\n",
    "```python\n",
    "e1 = (c*(a+b))/(c+d)\n",
    "e2 = (d*(a+b))/(c+d)\n",
    "```\n",
    "\n",
    "To make counting easier, we're introducing `Counter` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we import Counter, which is a special kind of dictionary that\n",
    "# is made for counting stuff.\n",
    "from collections import Counter\n",
    "\n",
    "# Try to understand how Counters work. Here is a toy example:\n",
    "c = Counter([1,1,1,2,2,3,3,4,5,6,7,8,8])\n",
    "\n",
    "# What do the following functions do? \n",
    "# - c.update()\n",
    "# - c.most_common()\n",
    "# - c.items()\n",
    "# - c.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You get this code for free :)\n",
    "counted_merity_lemmas = Counter(merity_lemmas)\n",
    "counted_weinberg_lemmas = Counter(weinberg_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say the `counted_merity_lemmas` is our Corpus 1, and `counted_weinberg_lemmas` is our Corpus 2. The values of `a` and `b` are simply the counts that are stored with the relevant word. `c` and `d` correspond to the sum of all values in `counted_merity_lemmas` and `counted_weinberg_lemmas`, respectively. \n",
    "\n",
    "**Exercise 9** Write the expected values function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# See above for explanation.\n",
    "def expected_values(word, corpus1, corpus2):\n",
    "    # Your code here\n",
    "    return e1, e2\n",
    "\n",
    "# The input looks like this:\n",
    "expected_values('the', counted_merity_lemmas, counted_weinberg_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In [their paper](http://ucrel.lancs.ac.uk/people/paul/publications/rg_acl2000.pdf), Rayson and Garside (2000) derive following the log likelihood formula:\n",
    "\n",
    "```python\n",
    "LL = 2*((a*log(a/e1)) + (b*log(b/e2)))\n",
    "```\n",
    "\n",
    "Well, that's almost directly translatable into Python! We only need to import log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10**: Write the log likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the expected_values function to compute e1 and e2 (Remember multiple assignment!)\n",
    "def log_likelihood(word, corpus1, corpus2):\n",
    "    # Your code here\n",
    "    return LL\n",
    "\n",
    "# The input looks like this:\n",
    "log_likelihood('the', counted_merity_lemmas, counted_weinberg_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Achieving significance & determining to which group a word belongs\n",
    "\n",
    "According to Rayson: \"The higher the [log likelihood] value, the more significant is the difference between two frequency scores. For these tables, a [log likelihood] of 3.8 or higher is significant at the level of p < 0.05.\" This basically means that **with a score higher than 3.8, we can reasonably assume that a word is 'typical' for one of the two corpora.**\n",
    "\n",
    "That still leaves us with one issue: determining the corpus the word is typical for. But that's easily solved: we just look at the *relative frequency* with which the word occurs in both corpora: **if a word has a high LL score, we say it is typical for corpus A if (a/c) > (b/d) (and vice versa for corpus B).** \n",
    "\n",
    "**Exercise 11** Create a function that returns two lists: words typical for corpus 1, and words typical for corpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def typical_words(corpus1, corpus2):\n",
    "    # Docstring here.\n",
    "    typical_for_corpus1 = []\n",
    "    typical_for_corpus2 = []\n",
    "    \n",
    "    # YOUR CODE HERE.\n",
    "    # NOTE: if a word only occurs in one of the two corpora, don't attempt to compute log likelihood.\n",
    "    # Just immediately append it to the relevant list.\n",
    "    \n",
    "    return typical_for_corpus1, typical_for_corpus2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the typical_words function to see which words are more typical\n",
    "# for merity_lemmas and which are more typical for weinberg_lemmas.\n",
    "\n",
    "typical_merity, typical_weinberg = typical_words(counted_merity_lemmas, \n",
    "                                                 counted_weinberg_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Explore the results here :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
