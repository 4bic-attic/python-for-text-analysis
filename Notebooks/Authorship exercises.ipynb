{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises in Authorship Detection\n",
    "\n",
    "This notebook provides a small introduction into authorship detection. It is by no means comprehensive, but there should be enough to get you started on this subject. **THIS IS A DRAFT**\n",
    "\n",
    "## Responsibility\n",
    "\n",
    "First, a point on responsibility. I want to make this very clear (especially since some of you might end up in Forensics): **Never blindly trust in a machine's judgment!** Computers are stupid, but they can easily impress humans with their ability to crunch numbers. Therefore, it's very important to manually inspect the data and the results as well. Try to understand *why* you got a particular result, and only then you can have any confidence in the conclusion.\n",
    "\n",
    "The methods illustrated in this notebook are by no means state-of-the-art, but they are foundational to the field of authorship detection.\n",
    "\n",
    "## Features\n",
    "\n",
    "By *features*, we mean properties or characteristics of a text that are useful inputs for a machine to make predictions about that text (in this case: predicting authorship). We recommend that you read [A survey of modern authorship attribution methods](http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full) by Efstathios Stamatatos. This paper provides a nice overview of features that are commonly used in authorship attribution. The paper dates back to 2008, and newer methods have been developed, but these features are still important. We'll work with a selection of features from the literature. Most important for us here is how to extract features *efficiently* and how to *represent* them. These are the features we will use:\n",
    "\n",
    "* Character counts\n",
    "* Mean word length\n",
    "* Mean sentence length\n",
    "* Standard deviation of sentence length\n",
    "* Type-token ratio\n",
    "* Hapax legomena ratio\n",
    "* Stopword counts\n",
    "* Whitespace\n",
    "\n",
    "Some of these features sound really basic, but don't let their basic nature fool you. [This piece on punctuation in novels](https://medium.com/@neuroecology/punctuation-in-novels-8f316d542ec4) shows you how informative punctuation can be. And here is an excellent quote by Gary Provost (from *100 Ways To Improve Your Writing*) that shows the power of sentence length to change the character of a text:\n",
    "\n",
    "> **VARY SENTENCE LENGTH**\n",
    "> \n",
    "> This sentence has five words. Here are five more words. Five-word sentences are fine. But several together become monotonous. Listen to what is happening. The writing is getting boring. The sound of it drones. It's like a stuck record. The ear demands some variety.\n",
    ">\n",
    "> Now listen. I vary the sentence length, and I create music. Music. The writing sings. It has a pleasant rhythm, a lilt, a harmony. I use short sentences. And I use sentences of medium length. And sometimes when I am certain the reader is rested, I will engage him with a sentence of considerable length, a sentence that burns with energy and builds with all the impetus of a crescendo, the roll of the drums, the crash of the cymbals--sounds that say listen to this, it is important.\n",
    ">\n",
    "> So write with a combination of short, medium, and long sentences. Create a sound that pleases the reader's ear. Don't just write words. Write music.\n",
    "\n",
    "(Fun fact: the use of sentence length to determine authorship goes back to [1939](http://www.jstor.org/stable/2332655?seq=1#page_scan_tab_contents)!)\n",
    "\n",
    "Of course, there are many more possible features. E.g. part-of-speech features (does the author use a lot of adjectives?) or n-gram features (what combinations of words does the author use?). Feel free to experiment with these after you've finished the exercises. Another approach is language modeling, e.g. using a Hidden Markov Model (HMM) or a Long Short-Term Memory network (LSTM). If you take this approach, the goal is to learn what sequences of tokens are typical for an author to use. With enough text, these models are really powerful.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Now let's get to work! The plan for this notebook is to write functions to extract the different features. However, we will not write a separate function for each feature! Instead, we'll group features that are naturally related to each other. Read the list of features again and try to think of the requirements each of them has: **what do you need to do in order to compute these features?**\n",
    "\n",
    "* Character counts\n",
    "* Mean word length\n",
    "* Mean sentence length\n",
    "* Median sentence length\n",
    "* Standard deviation of sentence length\n",
    "* Type-token ratio\n",
    "* Hapax legomena ratio\n",
    "* Stopword counts\n",
    "* Whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools: Counter\n",
    "\n",
    "We will make use of the `Counter`-object from the `collections` module in the standard library. `Counter` is a subclass of `dict`, and has all the functionality you would expect in a dictionary, plus some cool additions. Let's first import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to use it for counting.\n",
    "\n",
    "**Method 1**: Initialize the counter with an iterable object (string, list, set, tuple, generator). Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = Counter('This is a string whose characters will be counted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2**: After having initialized the counter, use the update method to increment the counts. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "cnt.update(['cat','dog','cat','cat'])\n",
    "print(cnt['cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How can you use `chars` to get the 10 most frequent characters? (Use the docs or the `dir` and `help` functions to find out.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What do you expect Python to do if you use `cnt['mouse']`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: The built-in Python method `sum()` returns a sum of the contents of whatever iterable it is given (given those contents correspond to ints or floats). So `sum([1,2,3])` returns `6`. How can you get the sum of all counts from a counter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools: Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "Now we'll write the functions to extract and combine all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting authorship\n",
    "\n",
    "Sit back and relax. Your job is done! Now let's see how well we can predict who wrote which text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python libraries\n",
    "\n",
    "There are already several libraries to perform authorship detection. Below is a short list of libraries that I've found while researching this subject. If you want to delve deeper into authorship detection, I'd recommend you look at these in more detail. (Even if you won't use these libraries, it's nice to see how others have tackled the problem. No need to reinvent the wheel!)\n",
    "\n",
    "* Mike Kestemont's [PyStyl](https://github.com/mikekestemont/pystyl) is probably the most complete library for authorship detection.\n",
    "* The Information Sciences Institute provides the [digStylometry](https://github.com/usc-isi-i2/dig-stylometry) package.\n",
    "\n",
    "## Miscellaneous scripts\n",
    "Here are some other scripts that I've used as inspiration for the exercises in this notebook.\n",
    "\n",
    "* [Stylometry](https://github.com/jpotts18/stylometry) is a small set of scripts from Jeff Potter.\n",
    "* [Here](https://github.com/d10genes/Authorship-Attribution) is another pair of scripts from 'Chris'.\n",
    "* [Here](http://www.aicbt.com/authorship-attribution/) is a blog from a consultancy company called AICBT Consulting."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
