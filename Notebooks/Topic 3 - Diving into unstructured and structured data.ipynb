{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 3 - Diving into unstructured and structured data\n",
    "\n",
    "\n",
    "\n",
    "This week, we will learn how to deal with unstructured and structured data in Python. But what exactly is the difference between them? Structured data is information with a high degree of organization, which can easily be ordered and processed by machines. You can compare it with a perfectly organized filing cabinet where everything is identified, labeled and easy to access. Examples of structured data formats are tables (CSV/TSV), XML, JSON and RDF. Unstructured data, however, is not organized in a pre-defined manner and therefore lacks structure. Examples are text documents (TXT, PDF, Word), but also bitmap images (GIF, JPEG, PNG, BMP).\n",
    "\n",
    "This notebook introduces some of these types of structured and unstructured data, and explains how you can work with files stored locally on your computer.\n",
    "\n",
    "**At the end of this week, you will be able to:**\n",
    "- open and read the contents of one or multiple `text` and `csv/tsv` files;\n",
    "- manipulate the content of files (e.g. sentence splitting, tokenization, POS-tagging, and lemmatization);\n",
    "- write new or manipulated content to new (or existing) files;\n",
    "- read and write JSON data; \n",
    "- import and use modules like `csv`, `json`, `nltk`, `os` and `glob`;\n",
    "- write a function.\n",
    "\n",
    "**This requires that you already have (some) knowledge about:**\n",
    "- basic objects: strings, lists and dictionaries;\n",
    "- for-loops;\n",
    "- boolean expressions;\n",
    "- if-statements.\n",
    "\n",
    "**If you want to learn more about these topics, you might find the following links useful:**\n",
    "- [Video: File Objects - Reading and Writing to Files](https://www.youtube.com/watch?v=Uh2ebFW8OYM)\n",
    "- [Video: Automate Parsing and Renaming of Multiple Files](https://www.youtube.com/watch?v=ve2pmm5JqmI)\n",
    "- [Video: OS Module - Use Underlying Operating System Functionality](https://www.youtube.com/watch?v=tJxcKyFMTGo)\n",
    "- [Video: Working With JSON](https://www.youtube.com/watch?v=Kf0q4Tf5M3c)\n",
    "- [Tutorial: The import statement](https://www.tutorialspoint.com/python/python_modules.htm)\n",
    "- [Tutorial: Defining Functions of your Own](http://anh.cs.luc.edu/python/hands-on/3.1/handsonHtml/functions.html)\n",
    "- [Tutorial: Reading and Manipulating CSV Files](https://newcircle.com/s/post/1572/python_for_beginners_reading_and_manipulating_csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Working with plain text files: Charlie & The Chocolate Factory\n",
    "\n",
    "When doing text analysis, you will often work files that contain plain text. These files typically end with the `.txt` extension. In Python, you can read the content of a file, store it as the type of object that you need (string, list, etc.) and manipulate it (e.g. replacing or removing words). You can also write new content to an existing or a new file.\n",
    "\n",
    "### 1.1 Reading (and closing) files\n",
    "Let's start with opening a file in Python. To do this, we need to associate the file on disk with a variable in Python. First, we tell Python where the file is stored on your disk. The location of your file is often referred to as the file path. Python will start looking in the 'working' or 'current' directory (which often will be where your Python script is). If it's in the working directory, you only have to tell Python the name of the file (e.g. `charlie.txt`). If it's not in the working directory, you have to tell Python the exact path to your file. We will create a string variable to store this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = \"../Data/Charlie/charlie.txt\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the double dots in the beginning of the file path; this means 'the parent of the current directory'. When writing a file path, you can use the following:\n",
    "- /     means the root of the current drive; \n",
    "- ./    means the current directory;\n",
    "- ../   means the parent of the current directory.  \t \n",
    "\n",
    "\n",
    "Now Python knows where your file is stored, we can open the file by using the built-in function `open()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "infile = open(filename, \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `open()` function requires the file path as its first argument. The second argument specifies the *mode* in which the file is opened. The mode you choose will depend on what you wish to do with the file. Here are some of our mode options:\n",
    "\n",
    "- 'r' : use for reading\n",
    "- 'w' : use for writing\n",
    "- 'x' : use for creating and writing to a new file\n",
    "- 'a' : use for appending to a file\n",
    "- 'r+' : use for reading and writing to the same file\n",
    "\n",
    "Let's now print `infile`. What do you think will happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Hey! That's not what I expected to happen!\", you might think. Python is not printing the contents of the file but only some mysterious mention of some `TextIOWrapper`. This `TextIOWrapper` thing is Python's way of saying it has *opened* a connection to the file `charlie.txt`. In order to *read* the contents of the file, Python provides three related operations. The first operation is `read()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content = infile.read()\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The variable `content` now holds the entire content of the file `charlie.txt` as a single string and we can access and manipulate it just like any other string. \n",
    "\n",
    "The second operation is `readlines()`, which returns a list of the lines in the file, where each item of the list represents a single line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = infile.readlines()\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, why doesn't this return anything? Something to keep in mind when you are reading from files is that once a file has been read using one of the `read' operations, it cannot be read again. Therefore, anytime you wish to read from a file you will have to open a new file variable. Let's try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile = open(filename, \"r\")\n",
    "lines = infile.readlines()\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can, for example, use a for-loop to print each line in the file (note that the second line is just a newline character):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    print(\"LINE:\", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third operation `readline()` returns the next line of the file, returning the text up to and including the next newline character (*\\n*). More simply put, this operation will read a file line-by-line. So if you call this operation again, it will return the next line in the file. Try it out below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile = open(filename, \"r\")\n",
    "print(infile.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(infile.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(infile.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading the contents of a file, the `TextWrapper` no longer needs to be open since we have stored the content as a variable. In fact, it is good practice to close the file as soon as you do not need it anymore. Now, lo and behold, we can achieve that with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is actually an easier (and preferred) way to make sure that the file is closed as soon as you don't need it anymore, namely using what is called a `context manager`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(filename) as infile:\n",
    "    content = infile.read()\n",
    "    \n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of using the with-statement is that it automatically closes the file once you leave the local context defined by the indentation level. If you 'manually' open and close the file, you risk forgetting to close the file. Therefore, context managers are considered a best-practice, and we will use the with-statement in all of our following code. \n",
    "\n",
    "**Exercise:** Write a program that opens `RedCircle.txt` in the `../Data/RedCircle` folder and prints its content as a single string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your program here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a program that opens `RedCircle.txt` in the `../Data/RedCircle` folder and prints the list containing all lines in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your program here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Manipulating the content of text files: importing and using the NLTK package\n",
    "Last week, we have done several exercises with manipulating strings. Let's recap. We have learned that some of the most common preprocessing steps are casefolding/lowercasing, punctuation removal and stemming/lemmatization. Did you know that there are some very useful NLP packages and modules to do some of these steps? One that is often used in text analysis is the Python package NLTK (the Natural Language Toolkit). However, to be able to use the modules that are part of the NLTK toolkit, we first need to *import* NLTK into our Python script. Let's quickly talk about importing packages and modules first.\n",
    "\n",
    "#### Importing modules\n",
    "\n",
    "Some things in Python, like `int`, `float` or `count()` are built-in and can be used whenever you want. But many things you will want to do need a little more than that. Try running the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now()\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you get a `NameError`. This is Python's way of telling you: \"I don't know what 'datetime' means, please tell me first.\" We can make the `datetime` module accessible by using a suitable `import` statement. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "current_time = datetime.datetime.now()\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second line, we basically say: \"from the package `datetime`, and its subpackage `datetime.datetime`, call the function `now()`.\" The [documentation](https://docs.python.org/3/reference/import.html) for the `import` statement gives more details about packages and modules. For example: \n",
    "\n",
    "> Python has only one type of module object, and all modules are of this type, regardless of whether the module is implemented in Python, C, or something else. To help organize modules and provide a naming hierarchy, Python has a concept of packages.\n",
    "\n",
    "> You can think of packages as the directories on a file system and modules as files within directories, but don’t take this analogy too literally since packages and modules need not originate from the file system. For the purposes of this documentation, we’ll use this convenient analogy of directories and files. Like file system directories, packages are organized hierarchically, and packages may themselves contain subpackages, as well as regular modules.\n",
    "\n",
    "> It’s important to keep in mind that all packages are modules, but not all modules are packages. Or put another way, packages are just a special kind of module. Specifically, any module that contains a __path__ attribute is considered a package.\n",
    "\n",
    "> All modules have a name. Subpackage names are separated from their parent package name by dots, akin to Python’s standard attribute access syntax. Thus you might have a module called sys and a package called `email`, which in turn has a subpackage called `email.mime` and a module within that subpackage called `email.mime.text`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we can also give the module a conventient name while importing it, which works as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "current_time = dt.datetime.now()\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's `from...import` statement lets you import specific attributes from a module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "current_time = datetime.now()\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization and sentence splitting\n",
    "Now we know how to import packages and modules, we can import the NLTK toolkit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amongst other things, the NLTK toolkit allows you to tokenize texts with the function `word_tokenize()`. To be able to use this function, we first need to download the NLTK Tokenizer Models. Run the following command to download a collection of NLTK data and models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try tokenizing our Charlie story! First, we will open and read the file again and assign the file contents to the variable `content`. Then, we can call the `word_tokenize()` function from the `nltk` module as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../Data/Charlie/charlie.txt\") as infile:\n",
    "    content = infile.read()\n",
    "\n",
    "tokens = nltk.word_tokenize(content)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we now have a list of all words in the text. The punctuation marks are also in the list, but as separate tokens. Another thing that NLTK can do for you is to split a text into sentences by using the `sent_tokenize()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(content)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do all sorts of cool things with these lists. For example, we can search for all words that have certain letters in them and add them to a list. Let's say we want to find all present participles in the text. We know that present participles end with *-ing*, so we can do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "present_participles = []\n",
    "for token in tokens:\n",
    "    if token.endswith(\"ing\"):\n",
    "        present_participles.append(token)\n",
    "print(present_participles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good! We now have a list of words like *boiling*, *sizzling*, etc. But wait... Oops, there is one word in the list that actually is not a present participle! Of course, also other words can end with *-ing*. So if we want to find all present participles, we have to come up with a smarter solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS tagging\n",
    "Once again, NLTK comes to the rescue. Using the function `pos_tag()`, we can label each word in the text with its part of speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of tuples. The first element of the tuple is the token, the second element indicates the part of speech of the token. This POS tagger uses the POS tag set of the Penn Treebank Project, which can be found [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). In this tag set, the `VBG` tag is used for present participles and gerunds. \n",
    "\n",
    "**Exercise:** Now let's try to make a list of all present participles in `charlie.txt` using the POS tags. Finish the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finish the following code:\n",
    "present_participles = []\n",
    "for token in tagged_tokens:\n",
    "    if token[1] == \"VBG\":\n",
    "        # ??\n",
    "print(present_participles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test your code here! If your code is correct, you should get a compliment :-)\n",
    "assert len(present_participles) == 11\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get the following list: ['boiling', 'bubbling', 'hissing', 'sizzling', 'clanking', 'running', 'hopping', 'knowing', 'rubbing', 'cackling', 'going']\n",
    "\n",
    "**Exercise:** Finish the following code to get *all* verbs. We already provided you with the full set of verb tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finish the following code:\n",
    "verb_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "verbs = []\n",
    "# Use a for-loop! \n",
    "\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test your code here! If your code is correct, you should get a compliment :-)\n",
    "assert len(verbs) == 39    \n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "We now have a list of all inflected forms of the verbs. We can also use NLTK to lemmatize words. We will use the WordNetLemmatizer for this. In the code below, we loop through the list of verbs, lemmatize each of the verbs, and add them to a new list called `verb_lemmas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "verb_lemmas = []\n",
    "for participle in verbs:\n",
    "    # For this lemmatizer, we need to indicate the POS of the word (in this case, v = verb)\n",
    "    lemma = lmtzr.lemmatize(participle, \"v\") \n",
    "    verb_lemmas.append(lemma)\n",
    "print(verb_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** The resulting list contains a lot of duplicates. Do you remember how you can get rid of these duplicates? Create a new list in which each verb occurs only once and name it `unique_verbs`. Then print the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test your code here! If your code is correct, you should get a compliment :-)\n",
    "assert len(unique_verbs) == 28    \n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Now use a for-loop to count the number of times that each of these verb lemmas occurs in the text! For each verb in the list you just created, get the count of this verb in `charlie.txt` using the `count()` method. Create a dictionary that contains the lemmas of the verbs as keys, and the counts of these verbs as values. Refer to the first Notebook if you forgot how to use the `count()` method or how to create dictionary entries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verb_counts = {}\n",
    "# Finish this for-loop\n",
    "for verb in unique_verbs:\n",
    "    # ??\n",
    "\n",
    "print(verb_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test your code here! If your code is correct, you should get a compliment :-)\n",
    "assert len(verb_counts) == 28 and verb_counts[\"bubble\"] == 1 and verb_counts[\"be\"] == 9\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Writing files\n",
    "\n",
    "So far, we have seen how to open a file, and how to read and manipulate its content. But you can also use Python to write files. Let's first slightly adapt our Charlie story by replacing the names in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "your_name = \"\" #type in your name \n",
    "friends_name = \"\" #type in the name of a friend \n",
    "new_content = content.replace(\"Charlie Bucket\", your_name)\n",
    "content = new_content.replace(\"Mr Wonka\", friends_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can open a new file and write the text to this file by using `write()` as follows (remember, we need to specify the mode in which we open the file, in this case the writing mode):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"../Data/Charlie/charlie_new.txt\"\n",
    "with open(filename, \"w\") as outfile:\n",
    "    outfile.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file in the folder 'texts' in any text editor and read a personalized version of the story!\n",
    "\n",
    "Let's try something else. Remember that we have a list of verb lemmas that occured in `charlie.txt`, which we assigned to the variable `verb_lemmas`. Let's say we want to write these lemmas to a file, with each lemma on a separate line. What do you think will happen if you run the following code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"../Data/Charlie/charlie_verbs.txt\"\n",
    "with open(filename, \"w\") as outfile:\n",
    "    outfile.write(verb_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you understand why you get this error?\n",
    "\n",
    "... exactly, you can only write strings to files. If you want to write the verbs that are stored within the list `verb_lemmas` to a file, you'll need to use a for-loop or create one string from the list using the `join()`-method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"../Data/Charlie/charlie_verbs.txt\"\n",
    "\n",
    "with open(filename, \"w\") as outfile:\n",
    "    \n",
    "    # Writing method 1\n",
    "    for verb in verb_lemmas:\n",
    "        outfile.write(verb)\n",
    "    outfile.write(\"\\n\\n\")\n",
    "\n",
    "    # Writing method 2 \n",
    "    for verb in verb_lemmas:\n",
    "        s = verb + \"\\n\"\n",
    "        outfile.write(s)\n",
    "    outfile.write(\"\\n\\n\")\n",
    "\n",
    "    # Writing method 3\n",
    "    s = \" \".join(verb_lemmas)\n",
    "    outfile.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the output of the following code in `charlie_verbs.txt` and try to understand the differences between the three writing methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(filename, \"r\") as infile:\n",
    "    content = infile.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a list containing 10 color names. Write each of these colors on a separate line to a file called `colors.txt` in the `../Data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your program here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CSV and TSV (tables): The 2016 Presidential Debate\n",
    "Now let's move on to structured data. There are several data structures, the most common of which is probably the *table*. A table represents a set of data points as a series of rows, with a column for each of the data points' properties. Tabular data can be encoded as CSV (comma-separated values) or TSV (tab-separated values). CSV and TSV files are simply plain text files in which each line represents a row and, within each line, a comma (for CSV) or a tab character (for TSV) separates the cells in the row (the columns).\n",
    "\n",
    "### 2.1 Reading CSV files\n",
    "Let's take a look at an example. The folder `text` contains a CSV file with the name `debate.csv`. This file contains transcripts of the 2016 (vice-)presidential debate from 26 September to 9 October. If you'd like, you can open this file in a text editor or Excel (convert text to columns by using the comma as delimiter) to see its content. In Python we could read this CSV file in a similar way as we have seen with plain text files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"../Data/Debate/debate.csv\"\n",
    "with open(filename, \"r\") as csvfile:\n",
    "    content = csvfile.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first line in this CSV file is the header indicating the names of the columns: `Line`, `Speaker`, `Text` and `Date`. The remaining lines represent the transcripts of the different speakers in chronological order. Let's say we want to create a list that contains each line of the CSV files, and each line itself is a list representing the different columns in the CSV file. We could do that by using the `readlines()` function that we have seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(filename, \"r\") as csvfile:\n",
    "    rows = csvfile.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are some problems with this approach. If we look at an example row (row 18), and try to split this row into its columns using a comma as separator, we see that we get 10 columns instead of 4. This is because Python has split the string by all occurrences of the comma, including the ones that are part of the transcripts (inside the `Text` column). In the CSV file, double quotation marks are used to surround the different cells (saying: \"treat the part between quotation marks as one unit\"), but the `split()` function does not take this into account and splits these units anyway:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_row = rows[18]\n",
    "columns = example_row.split(\",\")\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the quotation marks as well as end-of-line characters `\\n` are included in the strings representing the data inside the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better approach is to use a module built-in to Python which will simplify the parsing of CSV and TSV files. The module is called `csv`. First, we import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `csv` module, we can open and read the file as follows in Python: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"../Data/Debate/debate.csv\"\n",
    "with open(filename, \"r\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=\",\")\n",
    "print(csvreader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output, we created a `Reader` object that we assigned to the variable `csvreader`. A `Reader` object lets you iterate over lines in the CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(filename, \"r\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=\",\")\n",
    "    for row in csvreader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, using the `csv` module we are able to split the rows into columns using the comma separator, while keeping the units surrounded by double quotation marks intact! The quotation marks and the newline characters are also not part of the string anymore.\n",
    "\n",
    "In a way, the `Reader` object is similar to a list (rows) of lists (columns). We can also make this explicit by changing the type of `csvreader` to a list and assign that to a new variable `rows`, as shown in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"../Data/Debate/debate.csv\"\n",
    "with open(filename, \"r\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=\",\")\n",
    "    rows = list(csvreader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to the `Reader` object, this list will still be available after you close the file (in this case, outside of the context manager: the with-statement). Compare the following pieces of code and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in csvreader:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now access and manipulate each of (the properties of) the data points in the CSV file. For example, we can select and print only the first 5 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(rows[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or print only those rows where Trump is the speaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    speaker = row[1]\n",
    "    if speaker == \"Trump\":\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or print only those texts of the transcripts where Trump mentions Obama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    speaker = row[1]\n",
    "    text = row[2]\n",
    "    if speaker == \"Trump\" and \"Obama\" in text:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Now write a program to get all rows from the debate on the 9th of October 2016 that come from a different speaker than Trump or Clinton. Print both the speaker and the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Writing CSV files\n",
    "\n",
    "Writing CSV files using the `csv` module is just as easy as reading them. We can use `writer()` to create an object for writing, and then use `writerow()` or `writerows()` to write our data to the file. Whereas `writerow()` takes an iterable of cells to write, `writerows()` takes an iterable of iterables of cells to write. In other words, `writerow()` takes 1-dimensional data (one row), and `writerows()` takes 2-dimensional data (multiple rows).\n",
    "\n",
    "The following code takes the rows that we just read from `debate.csv` and writes them to a new file `debate.tsv` using a tab (`\\t`) as delimiter instead of a comma (check out the file that is now in the `Data/Debate` folder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outfilename = \"../Data/Debate/debate.tsv\"\n",
    "with open(outfilename, \"w\") as outfile:\n",
    "    csvwriter = csv.writer(outfile, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "    for row in rows:\n",
    "        csvwriter.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does exactly the same as the one above, but uses `writerows()` instead of `writerow()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outfilename = \"../Data/Debate/debate.tsv\"\n",
    "with open(outfilename, \"w\") as outfile:\n",
    "    csvwriter = csv.writer(outfile, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "    csvwriter.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have passed several keyword arguments into the `csv.writer()` function. We have set the `delimiter` to a tab character (the default is a comma), the `quotechar` to double quotes (which is also the default), and the `quoting` option to `csv.QUOTE_ALL` (the default is `csv.QUOTE_MINIMAL`. The `csv` module contains the following quoting options:\n",
    "\n",
    "| Option | Explanation |\n",
    "|-----------|--------|\n",
    "| `csv.QUOTE_ALL` |\tQuote everything, regardless of type |\n",
    "| `csv.QUOTE_MINIMAL` |\tQuote fields with special characters\t|  \t \n",
    "| `csv.QUOTE_NONNUMERIC` |\tQuote all fields that are not integers or floats  \t |\n",
    "| `csv.QUOTE_NONE` |\tDo not quote anything on output\t  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Now try to write a new `csv` file yourself! Create a CSV file called `friends.csv` in the `../Data/Friends` folder that contains the names of 5 of your friends, their gender, and their favorite animal. Do not quote anything on the output and use semicolons as separators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finish the following list\n",
    "csvdata = [\n",
    "    [\"First Name\", \"Last Name\", \"Gender\", \"Favorite animal\"], \n",
    "    # add 5 of your friends \n",
    "]\n",
    "\n",
    "# Create a CSV file called `friends.csv` and write the csv data to this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. JSON\n",
    "\n",
    "Let's have a look at another data format. You probably have heard about JSON before. JSON is a lightweight data-interchange format that is easy for humans to read and write, and easy for machines to parse and generate. It is completely language independent. However, data formatted in JSON is just like a Python dictionary! The `json` module provides an easy way to encode and decode data in JSON. This can be done with the following functions:\n",
    "\n",
    "- `json.load()` and `json.loads()` for reading JSON\n",
    "- `json.dump()` and `json.dumps()` for writing JSON \n",
    "\n",
    "The functions with an s take string arguments.\n",
    "\n",
    "We will show how JSON looks like and how to use these functions by creating a dictionary in Python called `dictionary_friends`. Recall from Topic 1 that dictionaries consist of keys and values. In this case, we have 4 keys (\"Chantal\", \"Jean\", \"Laura\" and \"Patrick\"), and the values of these keys are dictionaries themselves. These dictionaries have 6 keys (\"first name\", \"gender\", etc.) with strings, boolean values, integers and lists as values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary_friends = {\n",
    "    \"Chantal\": {\n",
    "        \"first name\": \"Chantal\", \n",
    "        \"last name\": \"van Son\", \n",
    "        \"gender\": \"female\", \n",
    "        \"age\": 26, \n",
    "        \"favorite_animal\": \"cat\",\n",
    "        \"single\": False,\n",
    "        \"siblings\": [\"Dennis\", \"Kelly\"]},\n",
    "    \"Jean\": {\n",
    "        \"first name\": \"Jean\", \n",
    "        \"last name\": \"van der Sluijs\", \n",
    "        \"gender\": \"male\", \n",
    "        \"age\": 30, \n",
    "        \"favorite_animal\": \"dog\",\n",
    "        \"single\": False,\n",
    "        \"siblings\": [\"Leo\"]},\n",
    "    \"Laura\": {\n",
    "        \"first name\": \"Laura\", \n",
    "        \"last name\": \"Kamphuis\", \n",
    "        \"gender\": \"female\", \n",
    "        \"age\": 25, \n",
    "        \"favorite_animal\": \"platypus\",\n",
    "        \"single\": False,\n",
    "        \"siblings\": [\"Danique\", \"Lisa\"]},\n",
    "    \"Patrick\": {\n",
    "        \"first name\": \"Patrick\", \n",
    "        \"last name\": \"van der Plas\", \n",
    "        \"gender\": \"male\", \n",
    "        \"age\": 26, \n",
    "        \"favorite_animal\": \"giraffe\",\n",
    "        \"single\": True,\n",
    "        \"siblings\": None}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's first import the `json` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can very easily write our dictionary to a file in JSON format by using `json.dump()` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Data/Friends/friends.json\", \"w\") as outfile:\n",
    "     json.dump(dictionary_friends, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `json.dump()` and not `json.dumps()` because we used a dictionary as argument, not a string. What happened here is that Python has turned the dictionary into a string in JSON format, and wrote this string to the file `friends.json`. We can read the file again to see how this string looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../Data/Friends/friends.json\", \"r\") as infile:\n",
    "    json_string = infile.read()\n",
    "    print(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks exactly like a Python dictionary! However, it really is a string. Remember, you can check the type of a Python object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(json_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare the JSON-encoded string to the original dictionary, there are some small differences. The boolean values, for example, are written as `true` and `false` instead of `True` and `False`, and `null` is equivalent to `None`.\n",
    "\n",
    "Here is how you turn a JSON-encoded string back into a Python dictionary (now we use a string as argument, so we use `json.loads()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary_friends = json.loads(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following code to check the output of `json.load()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dictionary_friends)\n",
    "print(type(dictionary_friends))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was easy, wasn't it? Next week, we will practice some more with JSON. \n",
    "\n",
    "**Exercises:**\n",
    "For now, let's practice a bit more with accessing the values of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example: This will print the gender of \"Chantal\"\n",
    "print(dictionary_friends[\"Chantal\"][\"gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the age of Jean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the first sibling of Laura\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write a for-loop and to print for each person his or her favorite animal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reading and writing multiple files: Vickie's dream reports\n",
    "\n",
    "So far, we have practiced with reading and writing single files containing data in different formats, including text, CSV/TSV and JSON. But you will often have multiple files to work with. The folder `../Data/Dreams` contains 10 text files describing dreams of Vickie, a 10-year-old girl. These texts are extracted from [DreamBank](http://www.dreambank.net/). We want to get a general idea of what Vickie's dreams are about. Therefore, we will extract all nouns from her dreams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 The `glob` and `os` modules\n",
    "To be able to process multiple files, we need to *iterate* over a list of files. These files are usually stored in one or multiple directories on your computer. In this case, we want to iterate over all the files in the directory `../Data/Dreams`. So we need to find a way to tell Python: \"I want to do something with all these files at this location!\" There are two modules that you can use for this: `glob` and `os`. \n",
    "\n",
    "#### The `glob` module\n",
    "Let's first look at the `glob` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `glob` module is very useful to find all the pathnames matching a specified pattern according to the rules used by the Unix shell. You can use two wildcards: the asterisk and the question mark. An asterisk (\\*) matches zero or more characters in a segment of a name. For example, the following code gives all filenames in the directory `./data/dreams`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for filename in glob.glob(\"../Data/Dreams/*\"):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! There is one file that, apparently, we should ignore. So actually, we only want to have those filenames in the directory that have the extension `.txt`. We can do that as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for filename in glob.glob(\"../Data/Dreams/*.txt\"):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A question mark (?) matches any single character in that position in the name. For example, the following code prints all filenames in the directory `./data/dreams` that start with 'vickie' followed by exactly 1 character and end with the extension `.txt` (note that this will not print `vickie10.txt`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for filename in glob.glob(\"../Data/Dreams/vickie?.txt\"):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find filenames recursively by using the pattern `\\*\\*` (the keyword argument `recursive` should be set to `True`), which will match any files and zero or more directories and subdirectories. The following code prints all files with the extension `.txt` in the directory `../Data` and in all its subdirectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for filename in glob.glob(\"../Data/**/*.txt\", recursive=True):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `os` module\n",
    "\n",
    "Another module that you will frequently see being used in examples is the `os` module. Let's have a look at this module to compare it with `glob`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `listdir()` method of the `os` module is similar to the `glob` method, but it does not use any wildcards. The following code will therefore return an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for filename in os.listdir(\"../Data/Dreams/*.txt\"):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we just use the directory without wildcards as argument. Then, we can use for example the `splitext` method of the module `os.path` to split the filename into the filename and its extension (returning a tuple). Then, we can use an `if-statement` if we want to search for only those filenames that have `.txt` as an extension. In the code below, we first print these tupes of filenames and their extensions, and then add the `.txt` files to a list called `txt_files`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txt_files = []\n",
    "for filename in os.listdir(\"../Data/Dreams\"):\n",
    "    \n",
    "    # Split into filename and extension; print the tuples\n",
    "    split_filename = os.path.splitext(filename)\n",
    "    print(split_filename)\n",
    "    \n",
    "    # Check if extension is .txt; if so, add to list\n",
    "    extension = os.path.splitext(filename)[1]\n",
    "    if extension == \".txt\":\n",
    "        txt_files.append(filename)\n",
    "\n",
    "print(\"\\nThis is the list of filenames with the extension `.txt`:\")\n",
    "print(txt_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, whereas the `glob` method returns a list of the full paths of each filename, the `listdir` method returns only a list of the filenames. If we want to have the full path, we need to join the path of the directory where the file is stored and the filename using the `join()` method of the `os.path` module as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for filename in os.listdir(\"../Data/Dreams\"):\n",
    "    extension = os.path.splitext(filename)[1]\n",
    "    if extension == \".txt\":\n",
    "        path_file = os.path.join(\"../Data/Dreams\", filename)\n",
    "        print(path_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This all seems a lot of extra work if you compare it to the `glob` module, and in a way it is :-) If you want to quickly find all the pathnames matching a specified pattern, the simple but powerful `glob` module is the way to go. However, the `os` module has many more features that can be very useful and which are not supported by the `glob` module. We will not go over each and every useful method here, but here's a list of some of the things that you can do (some of which we have seen above): \n",
    "- creating single or multiple directories: `os.mkdir()`, `os.mkdirs()`;\n",
    "- removing single or multiple directories: `os.rmdir()`, `os.rmdirs()`;\n",
    "- checking whether something is a file or a directory: `os.path.isfile()`, `os.path.isdir()`;\n",
    "- split a path and return a tuple containing the directory and filename: `os.path.split()`;\n",
    "- construct a pathname out of one or more partial pathnames: `os.path.join()`\n",
    "- split a filename and return a tuple containing the filename and the file extension: `os.path.splitext()`\n",
    "- get only the basename or the directory path: `os.path.basename()`, `os.path.dirname()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Writing a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that Python has several built-in functions. But you can also create your own function. A function is a reusable block of code that performs a specific task. Once you have defined a function, you can use it at any place in your Python script. You can even import a function in one script into another one in a similar way as we have seen before with importing modules. Therefore, they are very useful for tasks that you will perform more often. Plus, functions are a convenient way to order your code and make it more readable!\n",
    "\n",
    "Whenever you are writing a function, you need to think of the following things:\n",
    "- What is the purpose of the function?\n",
    "- How should I name the function?\n",
    "- What input does the function need?\n",
    "- What output should the function generate?\n",
    "\n",
    "We will use an example from [this website](http://anh.cs.luc.edu/python/hands-on/3.1/handsonHtml/functions.html) to show you some of the basics of writing a function. \n",
    "\n",
    "#### Purpose and name\n",
    "Let's say we want to sing a birthday song to Emily. Then we print the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Happy Birthday to you!\")\n",
    "print(\"Happy Birthday to you!\")\n",
    "print(\"Happy Birthday, dear Emily.\")\n",
    "print(\"Happy Birthday to you!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could be the purpose of a function: to print the lines of a birthday song for Emily. \n",
    "Now, we define a function to do this. We give the function a clear name, `happy_birthday_to_emily` and we define the function like below. Note that we specify exactly what it does in between triple quotation marks in the beginning of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def happy_birthday_to_emily():\n",
    "    \"\"\"\n",
    "    Prints a birthday song to Emily\n",
    "    \"\"\"\n",
    "    print(\"Happy Birthday to you!\")\n",
    "    print(\"Happy Birthday to you!\")\n",
    "    print(\"Happy Birthday, dear Emily.\")\n",
    "    print(\"Happy Birthday to you!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we execute the code above, we don't get any output. That's because we only told Python: \"Here's a function to do this, please remember it.\" If we actually want Python to execute everything insice this function, we have to *call* it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "happy_birthday_to_emily()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input: arguments\n",
    "But Emily is not the only one who celebrates her birthday once a year. To not exclude any of our friends, let's make a more generic function that can sing the song to anyone. This function will need as input the name of the person. The following function takes the name of the person (string) as an argument and then sings the song with the person’s name inserted at the end of the third line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def happy_birthday(name):\n",
    "    \"\"\"\n",
    "    Prints a birthday song with the \"name\" of the person inserted\n",
    "    \"\"\"\n",
    "    print(\"Happy Birthday to you!\")\n",
    "    print(\"Happy Birthday to you!\")\n",
    "    print(\"Happy Birthday, dear \" + name + \".\")\n",
    "    print(\"Happy Birthday to you!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to call this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "happy_birthday()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! We didn't specify the required positional argument `name`. Let's try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "happy_birthday(\"James\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output: the `return` statement\n",
    "If we call the function above, the print statements are executed. We can also create a string variable for the song within the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def happy_birthday(name):\n",
    "    song = \"\"\"Happy Birthday to you!\n",
    "Happy Birthday to you!\n",
    "Happy Birthday, dear %s.\n",
    "Happy Birthday to you!\"\"\" % name \n",
    "\n",
    "# Did you know you can format a string like this, using a percentage sign to insert a variable within a string?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we want to sing the song by printing the variable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing happens! We get an error that the variable `song` is not defined. That is because it is only defined *within* the function. Outside the scope of this function, the variable is not stored. If the variable is the *output* that we would like to get back from the function, we need to use a `return` statement as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def happy_birthday(name):\n",
    "    \"\"\"\n",
    "    Returns a birthday song as a string with the \"name\" of the person inserted\n",
    "    \"\"\"\n",
    "    song = \"\"\"Happy Birthday to you!\n",
    "Happy Birthday to you!\n",
    "Happy Birthday, dear %s.\n",
    "Happy Birthday to you!\"\"\" % name\n",
    "    return song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can call the function and assign its output to the variable `song`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song = happy_birthday(\"James\")\n",
    "print(song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have to give this variable the same name as the variable defined in the scope of the function. This works fine as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wohworhwffwk = happy_birthday(\"James\")\n",
    "print(wohworhwffwk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function can take multiple arguments as input, and can return multiple variables as output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sum_and_diff_len_strings(string1, string2):\n",
    "    \"\"\"\n",
    "    Returns the sum of and difference between the lengths of two strings\n",
    "    \"\"\"\n",
    "    sum_strings = len(string1) + len(string2)\n",
    "    diff_strings = len(string1) - len(string2)\n",
    "    return sum_strings, diff_strings\n",
    "\n",
    "sum_strings, diff_strings = sum_and_diff_len_strings(\"horse\", \"dog\")\n",
    "print(\"Sum:\", sum_strings)\n",
    "print(\"Difference:\", diff_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Now write a function that opens a file and returns its content as a single string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_content_file(filename):\n",
    "    # Finish this function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test your code here! If your code is correct, you should get a compliment :-)\n",
    "content = get_content_file(\"../Data/Dreams/vickie1.txt\")\n",
    "assert(len(content) == 751)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.3 Putting it all together\n",
    "Now we know how to process multiple files and how to write a function. Let's try to use these skills to get all nouns from Vickie's dream reports! Remember how we tagged all tokens with their POS tags on a single text file? We had to open the file, read the contents, tokenize the text, and use the POS tagger (remember, we first needed to import `nltk` to be able to use it). we can now write a single function that does all that for us. The following function reads the specified file and returns the tokens with their POS tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def tag_tokens_file(filename):\n",
    "    \"Read the contents of FILENAME and returns a list of its tokens with their POS tags.\"\n",
    "    with open(filename) as infile:\n",
    "        content = infile.read()\n",
    "        tokens = nltk.word_tokenize(content)\n",
    "        tagged_tokens = nltk.pos_tag(tokens)\n",
    "    return tagged_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of having to open a file, read the contents and close the file, we can just call the function `tag_tokens_file` to do this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"../Data/Dreams/vickie1.txt\"\n",
    "tagged_tokens = tag_tokens_file(filename)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this for each of the files in the `../Data/Dreams` directory by using a for-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Iterate over the `.txt` files in the directory and perform POS tagging on each of them\n",
    "for filename in glob.glob(\"../Data/Dreams/*.txt\"): \n",
    "    tagged_tokens = tag_tokens_file(filename)\n",
    "    print(filename, \"\\n\", tagged_tokens, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we extend this code a bit so that we don't print all POS-tagged tokens of each file, but we get all (proper) nouns from the texts and add them to a list called `nouns_in_dreams`. Then, we print the set of nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a list that will contain all nouns\n",
    "nouns_in_dreams = []\n",
    "\n",
    "# Iterate over the `.txt` files in the directory and perform POS tagging on each of them\n",
    "for filename in glob.glob(\"../Data/Dreams/*.txt\"): \n",
    "    tagged_tokens = tag_tokens_file(filename)\n",
    "        \n",
    "    # Get all (proper) nouns in the text (\"NN\" and \"NNP\") and add them to the list\n",
    "    for token in tagged_tokens:\n",
    "        if token[1] in [\"NN\", \"NNP\"]:\n",
    "            nouns_in_dreams.append(token[0])\n",
    "\n",
    "# Print the set of nouns in all dreams\n",
    "print(set(nouns_in_dreams))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an idea what Vickie dreams about!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
